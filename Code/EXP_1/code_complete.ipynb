{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting peptide antibiofilm potential\n",
    "Reference article: https://github.com/davidanastasiu/antibiofilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Pickle package\n",
    "import pickle\n",
    "\n",
    "# Packages for visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(font_scale=1.2)\n",
    "#Ml\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for Exp 1 - peptides linked with Homo sapiens immune system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset \n",
    "peptides = pd.read_csv('meus_peptides.csv', encoding='cp1252') \n",
    "peptides = peptides.round(3)\n",
    "peptides= peptides.dropna(subset=peptides.columns[3:]) # drop NA value if they exists\n",
    "peptides\n",
    "\n",
    "#more negative samples taht positive\n",
    "#capacity antibiofilm type=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many peptides associate to a positive class- antibiofilm capacity\n",
    "count = sum(peptides['Type'] == 1)\n",
    "count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, i.e. the features or attributes\n",
    "characters=peptides.to_numpy() \n",
    "print(characters)\n",
    "#each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, i.e. the class attribute where 0=negative and 1=positive\n",
    "type_label=peptides['Type'].to_numpy()\n",
    "print(type_label)\n",
    "#the type existent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common mapping\n",
    "X=characters #rest of data\n",
    "Y=type_label #0 or 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset 80/20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test splitting such that 80% of the dataset goes to training and 20% to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, Y, stratify=Y,  test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_te.shape) #to confirm the correct division\n",
    "print(y_te.shape) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data between 0-min and 1-max\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratitified k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedKFold(n_splits =10, random_state=42, shuffle=True) #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper optimization\n",
    "parameters = {\n",
    "    'C': [0.1, 1, 10,100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 0.01, 0.001]\n",
    "}\n",
    "svm_classifier = SVC()\n",
    "grid_search = GridSearchCV(svm_classifier, parameters, scoring='f1',cv=5)\n",
    "grid_search.fit(X_tr[:,3:], y_tr)\n",
    "best_params = grid_search.best_params_\n",
    "best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_classifier = svm.SVC(kernel='linear', C=0.1, gamma=0.01,probability=True) #hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in each split we save some metrics\n",
    "sum_SS_f1=0 \n",
    "scores_ss = [] \n",
    "mccs_ss = []\n",
    "f1s_ss = []\n",
    "n=0\n",
    "for train_index, test_index in sss.split(X_tr, y_tr):\n",
    "    positive=0\n",
    "    negative=0\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        if (y_tr[i]):\n",
    "            positive=positive+1\n",
    "        else:\n",
    "            negative=negative+1\n",
    "    print('positive sample', positive)\n",
    "    print('negative sample', negative)\n",
    "    \n",
    "    X_SS_train, X_SS_test, y_SS_train, y_SS_test = X_tr[train_index,3:], X_tr[test_index,3:], y_tr[train_index], y_tr[test_index]\n",
    "    \n",
    "\n",
    "\n",
    "    X_SS_train= min_max_scaler.fit_transform(X_SS_train)\n",
    "   \n",
    "\n",
    "    X_SS_test = min_max_scaler.transform(X_SS_test)\n",
    "   \n",
    "\n",
    "    SS_classifier.fit(X_SS_train, y_SS_train)\n",
    "    scores_ss.append(SS_classifier.score(X_SS_test, y_SS_test))\n",
    "    ypred=(SS_classifier.predict(X_SS_test))\n",
    "    mcc=matthews_corrcoef(y_SS_test, ypred)\n",
    "    mccs_ss.append (mcc)\n",
    "    f1=f1_score(y_SS_test, ypred)\n",
    "    f1s_ss.append(f1)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print(\"Scores: \",np.min(scores_ss), np.max(scores_ss), np.std(scores_ss))\n",
    "print(\"F1s: \", np.min(f1s_ss), np.max(f1s_ss), np.std(f1s_ss))\n",
    "print(\"MCCs: \", np.min(mccs_ss), np.max(mccs_ss), np.std(mccs_ss))\n",
    "print ( \"avg cross-validation accuracy:\", (sum(scores_ss)/10))\n",
    "print ( \"avg cross-validation f1:\", (sum(f1s_ss)/10))\n",
    "print ( \"avg cross-validation mcc:\", (sum(mccs_ss)/10))\n",
    "print(\"*************************************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and see the performance of SVM model in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_tr[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "SS_classifier.fit(X_new,y_tr)\n",
    "y_tr_predict = SS_classifier.predict(X_new)\n",
    "\n",
    "print('f1 on Train set: ', f1_score(y_tr, y_tr_predict))\n",
    "print('MCC on Train set: ', matthews_corrcoef(y_tr, y_tr_predict))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_tr, y_tr_predict).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Train set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Train set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Train set: ', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and see the performance of SVM model in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_new = X_te[:,3:]\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "y_SS_pred=SS_classifier.predict(X_te_new)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_SS_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_SS_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_SS_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_te, y_SS_pred)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "RF_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(RF_classifier, parameters, scoring='f1', cv=5)\n",
    "\n",
    "grid_search.fit(X_tr[:, 3:], y_tr)\n",
    "\n",
    "best_params_RF = grid_search.best_params_\n",
    "best_params_RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_classifier = RandomForestClassifier(random_state=42, max_depth=None, min_samples_split=5, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_SS_f1=0 \n",
    "scores_rf = [] \n",
    "mccs_rf = [] \n",
    "f1s_rf = []\n",
    "n=0\n",
    "\n",
    "for train_index, test_index in sss.split(X_tr, y_tr):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        if y_tr[i] == 1:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print('Positive samples:', positive)\n",
    "    print('Negative samples:', negative)\n",
    "    \n",
    "    X_RF_train, X_RF_test, y_RF_train, y_RF_test = X_tr[train_index,3:], X_tr[test_index,3:], y_tr[train_index], y_tr[test_index]\n",
    "    \n",
    "    X_RF_train_new = X_RF_train\n",
    "    X_RF_train_new = min_max_scaler.fit_transform(X_RF_train_new)\n",
    "    \n",
    "    X_RF_test_new = X_RF_test\n",
    "    X_RF_test_new = min_max_scaler.transform(X_RF_test_new)\n",
    "    \n",
    "    RF_classifier.fit(X_RF_train_new, y_RF_train)\n",
    "    scores_rf.append(RF_classifier.score(X_RF_test_new, y_RF_test))\n",
    "    y_pred = RF_classifier.predict(X_RF_test_new)\n",
    "    mcc = matthews_corrcoef(y_RF_test, y_pred)\n",
    "    mccs_rf.append(mcc)\n",
    "    f1 = f1_score(y_RF_test, y_pred)\n",
    "    f1s_rf.append(f1)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print(\"Scores: \", np.min(scores_rf), np.max(scores_rf), np.std(scores_rf))\n",
    "print(\"F1s: \", np.min(f1s_rf), np.max(f1s_rf), np.std(f1s_rf))\n",
    "print(\"MCCs: \", np.min(mccs_rf), np.max(mccs_rf), np.std(mccs_rf))\n",
    "print(\"avg cross-validation accuracy:\", (sum(scores_rf) / 10))\n",
    "print(\"avg cross-validation f1:\", (sum(f1s_rf) / 10))\n",
    "print(\"avg cross-validation mcc:\", (sum(mccs_rf) / 10))\n",
    "print(\"*************************************\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train and see the performance of RF model in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_tr[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "RF_classifier.fit(X_new, y_tr)\n",
    "y_tr_predict = RF_classifier.predict(X_new)\n",
    "\n",
    "print('f1 on Train set: ', f1_score(y_tr, y_tr_predict))\n",
    "print('MCC on Train set: ', matthews_corrcoef(y_tr, y_tr_predict))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_tr, y_tr_predict).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Train set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Train set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Train set: ', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test and see the performance of RF model in test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_new = X_te[:,3:]\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "y_RF_pred=RF_classifier.predict(X_te_new)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_RF_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_RF_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_RF_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_te, y_RF_pred)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'var_smoothing': [1e-12,1e-10,1e-9, 1e-8, 1e-7,1e-5]\n",
    "}\n",
    "\n",
    "NB_classifier = GaussianNB()\n",
    "\n",
    "grid_search = GridSearchCV(NB_classifier, parameters, scoring='f1', cv=5)\n",
    "grid_search.fit(X_tr[:, 3:], y_tr)\n",
    "\n",
    "best_params_NB = grid_search.best_params_\n",
    "best_params_NB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_classifier = GaussianNB(var_smoothing=1e-12)\n",
    "\n",
    "sum_nb_f1=0 \n",
    "scores_nb = [] \n",
    "mccs_nb = [] \n",
    "f1s_nb = []\n",
    "n=0\n",
    "\n",
    "for train_index, test_index in sss.split(X_tr, y_tr):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        if y_tr[i] == 1:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print('Positive samples:', positive)\n",
    "    print('Negative samples:', negative)\n",
    "    \n",
    "    X_NB_train, X_NB_test, y_NB_train, y_NB_test = X_tr[train_index,3:], X_tr[test_index,3:], y_tr[train_index], y_tr[test_index]\n",
    "    \n",
    "    X_NB_train_new = X_NB_train\n",
    "    X_NB_train_new = min_max_scaler.fit_transform(X_NB_train_new)\n",
    "    \n",
    "    X_NB_test_new = X_NB_test\n",
    "    X_NB_test_new = min_max_scaler.transform(X_NB_test_new)\n",
    "    \n",
    "    NB_classifier.fit(X_NB_train_new, y_NB_train)\n",
    "    scores_nb.append(NB_classifier.score(X_NB_test_new, y_NB_test))\n",
    "    y_pred = NB_classifier.predict(X_NB_test_new)\n",
    "    mcc = matthews_corrcoef(y_NB_test, y_pred)\n",
    "    mccs_nb.append(mcc)\n",
    "    f1 = f1_score(y_NB_test, y_pred)\n",
    "    f1s_nb.append(f1)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print(\"Scores: \", np.min(scores_nb), np.max(scores_nb), np.std(scores_nb))\n",
    "print(\"F1s: \", np.min(f1s_nb), np.max(f1s_nb), np.std(f1s_nb))\n",
    "print(\"MCCs: \", np.min(mccs_nb), np.max(mccs_nb), np.std(mccs_nb))\n",
    "print(\"avg cross-validation accuracy:\", (sum(scores_nb) / 10))\n",
    "print(\"avg cross-validation f1:\", (sum(f1s_nb) / 10))\n",
    "print(\"avg cross-validation mcc:\", (sum(mccs_nb) / 10))\n",
    "print(\"*************************************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and see the performance of NB model in train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_tr[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "NB_classifier.fit(X_new, y_tr)\n",
    "y_tr_predict = NB_classifier.predict(X_new)\n",
    "\n",
    "print('f1 on Train set: ', f1_score(y_tr, y_tr_predict))\n",
    "print('MCC on Train set: ', matthews_corrcoef(y_tr, y_tr_predict))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_tr, y_tr_predict).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Train set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Train set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Train set: ', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and see the performance of NB model in test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_new = X_te[:,3:]\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "y_NB_pred = NB_classifier.predict(X_te_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_NB_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_NB_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_NB_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_te, y_NB_pred)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dt_classifier, parameters, scoring='f1', cv=5)\n",
    "grid_search.fit(X_tr[:, 3:], y_tr)\n",
    "\n",
    "best_params_DT = grid_search.best_params_\n",
    "best_params_DT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_classifier = DecisionTreeClassifier(random_state=42, criterion='gini', max_depth=5, min_samples_leaf=4, min_samples_split=2)\n",
    "\n",
    "sum_dt_f1=0 \n",
    "scores_dt = [] \n",
    "mccs_dt = [] \n",
    "f1s_dt = []\n",
    "\n",
    "for train_index, test_index in sss.split(X_tr, y_tr):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        if y_tr[i] == 1:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print('Positive samples:', positive)\n",
    "    print('Negative samples:', negative)\n",
    "    \n",
    "    X_DT_train, X_DT_test, y_DT_train, y_DT_test = X_tr[train_index,3:], X_tr[test_index,3:], y_tr[train_index], y_tr[test_index]\n",
    "    \n",
    "    X_DT_train_new = X_DT_train\n",
    "    X_DT_train_new = min_max_scaler.fit_transform(X_DT_train_new)\n",
    "    \n",
    "    X_DT_test_new = X_DT_test\n",
    "    X_DT_test_new = min_max_scaler.transform(X_DT_test_new)\n",
    "    \n",
    "    DT_classifier.fit(X_DT_train_new, y_DT_train)\n",
    "    scores_dt.append(DT_classifier.score(X_DT_test_new, y_DT_test))\n",
    "    y_pred = DT_classifier.predict(X_DT_test_new)\n",
    "    mcc = matthews_corrcoef(y_DT_test, y_pred)\n",
    "    mccs_dt.append(mcc)\n",
    "    f1 = f1_score(y_DT_test, y_pred)\n",
    "    f1s_dt.append(f1)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print(\"Scores: \", np.min(scores_dt), np.max(scores_dt), np.std(scores_dt))\n",
    "print(\"F1s: \", np.min(f1s_dt), np.max(f1s_dt), np.std(f1s_dt))\n",
    "print(\"MCCs: \", np.min(mccs_dt), np.max(mccs_dt), np.std(mccs_dt))\n",
    "print(\"avg cross-validation accuracy:\", (sum(scores_dt) / 10))\n",
    "print(\"avg cross-validation f1:\", (sum(f1s_dt) / 10))\n",
    "print(\"avg cross-validation mcc:\", (sum(mccs_dt) / 10))\n",
    "print(\"*************************************\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and see the performance of DT model in train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new =X_tr[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "DT_classifier.fit(X_new, y_tr)\n",
    "y_tr_predict = DT_classifier.predict(X_new)\n",
    "\n",
    "print('f1 on Train set: ', f1_score(y_tr, y_tr_predict))\n",
    "print('MCC on Train set: ', matthews_corrcoef(y_tr, y_tr_predict))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_tr, y_tr_predict).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Train set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Train set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Train set: ', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test and see the performance of DT model in test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_new = X_te[:,3:]\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "y_DT_pred = DT_classifier.predict(X_te_new)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_DT_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_DT_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_DT_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_te, y_DT_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'penalty': ['l1', 'l2', 'none'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "    'max_iter': [10,100, 200]\n",
    "}\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "grid_search = GridSearchCV(logistic_regression, parameters, scoring='f1', cv=5)\n",
    "\n",
    "grid_search.fit(X_tr[:, 3:], y_tr)\n",
    "\n",
    "best_params_LT = grid_search.best_params_\n",
    "best_params_LT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_classifier = LogisticRegression(random_state=42, C=1, max_iter=10, penalty='l1', solver='liblinear')\n",
    "\n",
    "sum_lr_f1=0 \n",
    "scores_lr = [] \n",
    "mccs_lr = [] \n",
    "f1s_lr = []\n",
    "\n",
    "for train_index, test_index in sss.split(X_tr, y_tr):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        if y_tr[i] == 1:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print('Positive samples:', positive)\n",
    "    print('Negative samples:', negative)\n",
    "    \n",
    "    X_LR_train, X_LR_test, y_LR_train, y_LR_test =  X_tr[train_index,3:], X_tr[test_index,3:], y_tr[train_index], y_tr[test_index]\n",
    "    \n",
    "    X_LR_train_new = X_LR_train\n",
    "    X_LR_train_new = min_max_scaler.fit_transform(X_LR_train_new)\n",
    "    \n",
    "    X_LR_test_new = X_LR_test\n",
    "    X_LR_test_new = min_max_scaler.transform(X_LR_test_new)\n",
    "    \n",
    "    LR_classifier.fit(X_LR_train_new, y_LR_train)\n",
    "    scores_lr.append(LR_classifier.score(X_LR_test_new, y_LR_test))\n",
    "    y_pred = LR_classifier.predict(X_LR_test_new)\n",
    "    mcc = matthews_corrcoef(y_LR_test, y_pred)\n",
    "    mccs_lr.append(mcc)\n",
    "    f1 = f1_score(y_LR_test, y_pred)\n",
    "    f1s_lr.append(f1)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print(\"Scores: \", np.min(scores_lr), np.max(scores_lr), np.std(scores_lr))\n",
    "print(\"F1s: \", np.min(f1s_lr), np.max(f1s_lr), np.std(f1s_lr))\n",
    "print(\"MCCs: \", np.min(mccs_lr), np.max(mccs_lr), np.std(mccs_lr))\n",
    "print(\"avg cross-validation accuracy:\", (sum(scores_lr) / 10))\n",
    "print(\"avg cross-validation f1:\", (sum(f1s_lr) / 10))\n",
    "print(\"avg cross-validation mcc:\", (sum(mccs_lr) / 10))\n",
    "print(\"*************************************\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and see the performance of LR model in train dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_tr[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "LR_classifier.fit(X_new, y_tr)\n",
    "y_tr_predict = LR_classifier.predict(X_new)\n",
    "\n",
    "print('f1 on Train set: ', f1_score(y_tr, y_tr_predict))\n",
    "print('MCC on Train set: ', matthews_corrcoef(y_tr, y_tr_predict))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_tr, y_tr_predict).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Train set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Train set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Train set: ', accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test and see the performance of SVM model in test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_new = X_te[:,3:]\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "y_LR_pred = LR_classifier.predict(X_te_new)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_LR_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_LR_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_LR_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_te, y_LR_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SVM\n",
    "SS_classifier.fit(X_tr[:,3:], y_tr)\n",
    "y_svm_pred_prob = SS_classifier.predict_proba(X_te[:,3:])[:, 1]\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_te, y_svm_pred_prob)\n",
    "auc_svm = roc_auc_score(y_te, y_svm_pred_prob)\n",
    "\n",
    "# # Random Forest~\n",
    "# RF_classifier.fit(X_tr[:,3:], y_tr)\n",
    "# y_rf_pred_prob = RF_classifier.predict_proba(X_te[:,3:])[:, 1]\n",
    "# fpr_rf, tpr_rf, _ = roc_curve(y_te, y_rf_pred_prob)\n",
    "# auc_rf = roc_auc_score(y_te, y_rf_pred_prob)\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Naive Bayes\n",
    "# NB_classifier.fit(X_tr[:,3:], y_tr)\n",
    "# y_nb_pred_prob = NB_classifier.predict_proba(X_te[:,3:])[:, 1]\n",
    "# fpr_nb, tpr_nb, _ = roc_curve(y_te, y_nb_pred_prob)\n",
    "# auc_nb = roc_auc_score(y_te, y_nb_pred_prob)\n",
    "\n",
    "# # Decision Tree\n",
    "# DT_classifier.fit(X_tr[:,3:], y_tr)\n",
    "# y_dt_pred_prob = DT_classifier.predict_proba(X_te[:,3:])[:, 1]\n",
    "# fpr_dt, tpr_dt, _ = roc_curve(y_te, y_dt_pred_prob)\n",
    "# auc_dt = roc_auc_score(y_te, y_dt_pred_prob)\n",
    "\n",
    "# # Logistic Regression\n",
    "# LR_classifier.fit(X_tr[:,3:], y_tr)\n",
    "# y_lr_pred_prob = LR_classifier.predict_proba(X_te[:,3:])[:, 1]\n",
    "# fpr_lr, tpr_lr, _ = roc_curve(y_te, y_lr_pred_prob)\n",
    "# auc_lr = roc_auc_score(y_te, y_lr_pred_prob)\n",
    "\n",
    "# Plotando as curvas ROC\n",
    "plt.plot(fpr_svm, tpr_svm, label='SVM (AUC = %0.2f)' % auc_svm)\n",
    "#plt.plot(fpr_rf, tpr_rf, label='Random Forest (AUC = %0.2f)' % auc_rf)\n",
    "#plt.plot(fpr_nb, tpr_nb, label='Naive Bayes (AUC = %0.2f)' % auc_nb)\n",
    "#plt.plot(fpr_dt, tpr_dt, label='Decision Tree (AUC = %0.2f)' % auc_dt)\n",
    "#plt.plot(fpr_lr, tpr_lr, label='Logistic Regression (AUC = %0.2f)' % auc_lr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Linha diagonal para referência\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic-ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset - sintethic e stample peptides\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best model is SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the best model, we analyze its predictive capacity in a dataset that has not been trained (concern with overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model \n",
    "SS_classifier = svm.SVC(kernel='linear', C=0.1, gamma=0.01,probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataset\n",
    "peptides_novo = pd.read_csv('meus_peptides_dataset2.csv', encoding='cp1252') #latin encoding\n",
    "peptides_novo = peptides_novo.round(3)\n",
    "peptides_novo= peptides_novo.dropna(subset=peptides.columns[3:])\n",
    "peptides_novo\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new datset and see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "SS_classifier.fit(X_new, Y)\n",
    "X_peptides = peptides_novo.to_numpy()[:, 3:]  \n",
    "type_label_2=peptides_novo['Type'].to_numpy()\n",
    "Y_novo=type_label_2\n",
    "X_peptides_normalizado = min_max_scaler.transform(X_peptides)\n",
    "y_novo_pred=SS_classifier.predict(X_peptides_normalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(Y_novo, y_novo_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(Y_novo, y_novo_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(Y_novo, y_novo_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(Y_novo, y_novo_pred)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repurposing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new dataset with anticacer, insecticidial, antiviral peptides\n",
    "Note:this dataset was treated in exp2 archive\n",
    "can this peptides also be linked with antibiofilm capcity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides_3 = pd.read_csv('anti_viral_cancer_parasita_ins.csv', encoding='cp1252') #latin encoding\n",
    "peptides_3 = peptides_3.round(3)\n",
    "peptides_3= peptides_3.dropna(subset=peptides.columns[3:])\n",
    "peptides_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_classifier.fit(X_new, Y)\n",
    "X_peptides_3 = peptides_3.to_numpy()[:, 3:]  \n",
    "X_peptides_normalizado_3 = min_max_scaler.transform(X_peptides_3)\n",
    "y_peptides_predict = SS_classifier.predict(X_peptides_normalizado_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(y_peptides_predict)\n",
    "\n",
    "colors = ['blue', 'green']\n",
    "\n",
    "plt.bar([0, 1], counts, color=colors)\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "legend_labels = ['No Repurposing', 'Repurposing']\n",
    "legend_handles = [plt.bar([0], [0], color=colors[i])[0] for i in range(len(colors))]\n",
    "plt.legend(legend_handles, legend_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guilherme Sousa; Anália lourenço; Maria oliveira"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
